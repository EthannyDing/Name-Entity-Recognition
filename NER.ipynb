{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sys import exit\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, concatenate, Dense,TimeDistributed, Dropout, Bidirectional\n",
    "import keras\n",
    "#from keras.utils.vis_utils import plot_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import Sequence\n",
    "from keras.utils import to_categorical\n",
    "from seqeval.metrics import (classification_report, f1_score, precision_score, recall_score, accuracy_score)\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory C:\\Users\\Ethan\n",
      "Directory changed\n"
     ]
    }
   ],
   "source": [
    "# set working directory as ''\n",
    "# import text\n",
    "print(\"Current working directory\", os.getcwd())\n",
    "try:\n",
    "    # Change the current working Directory    \n",
    "    os.chdir(\"C:\\\\Users\\\\Ethan\\\\Desktop\\\\Codetest\")\n",
    "    print(\"Directory changed\")\n",
    "except OSError:\n",
    "    print(\"Can't change the Current Working Directory\") \n",
    "    \n",
    "text_dir = 'news_tagged_data.txt'\n",
    "word2vec_dir = 'wordvecs.txt'\n",
    "with open(text_dir, 'r') as f:\n",
    "    text = f.read()\n",
    "f.close()\n",
    "\n",
    "words = []\n",
    "tags = []\n",
    "for st in text.split('\\n'):\n",
    "    try:\n",
    "        tags.append(st.split(\"\\t\")[1])\n",
    "        words.append(st.split(\"\\t\")[0])\n",
    "    except IndexError:\n",
    "        tags.append('O')\n",
    "        words.append('padword')\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1437 words in the text\n",
      "9 types of tags\n"
     ]
    }
   ],
   "source": [
    "# check corpus and tags, create dictionaries of tag2num and num2tag\n",
    "corpus = list(set(words))\n",
    "n_tags = list(set(tags))\n",
    "\n",
    "tag2num = dict([tag, i] for i, tag in enumerate(n_tags))\n",
    "num2tag = dict([i, tag] for i, tag in enumerate(n_tags))\n",
    "\n",
    "print(\"{:} words in the text\".format(len(corpus)))\n",
    "print(\"{:} types of tags\".format(len(n_tags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spotlight',\n",
       " 'news',\n",
       " 'that',\n",
       " 's',\n",
       " 'centered',\n",
       " 'around',\n",
       " 'a',\n",
       " 'fire',\n",
       " 'padword']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get tokens of sentence and convert labels to numbers\n",
    "def form_sentences(word_list, tag_list):\n",
    "    sentences = [[]]\n",
    "    sen_tag = [[]]\n",
    "    last = len(word_list) - 1    \n",
    "    for i, (w, t) in enumerate(zip(word_list, tag_list)):\n",
    "        if w == \"padword\":\n",
    "            sentences[-1].append(\"padword\")\n",
    "            sen_tag[-1].append(tag2num[\"O\"])\n",
    "            if i != last:\n",
    "                sentences.append([])\n",
    "                sen_tag.append([])\n",
    "        else:\n",
    "            sentences[-1].append(w)\n",
    "            sen_tag[-1].append(tag2num[t])\n",
    "    return sentences, sen_tag\n",
    "sentences, sen_tag = form_sentences(words, tags)\n",
    "sentences[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 7, 0, 0, 0, 0, 0, 5, 0]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen_tag[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest sentence contains 30 words\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get the distribution of sentences length\n",
    "sen_lengths = [len(sen) for sen in sentences]\n",
    "plt.hist(sen_lengths)\n",
    "print(\"The longest sentence contains {:} words\".format(np.max(sen_lengths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spotlight',\n",
       " 'news',\n",
       " 'that',\n",
       " 's',\n",
       " 'centered',\n",
       " 'around',\n",
       " 'a',\n",
       " 'fire',\n",
       " 'padword',\n",
       " 'padword',\n",
       " 'padword',\n",
       " 'padword',\n",
       " 'padword',\n",
       " 'padword',\n",
       " 'padword',\n",
       " 'padword',\n",
       " 'padword',\n",
       " 'padword',\n",
       " 'padword']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I'll choose the 95 percentile of sentence length vector as max_len for sequence padding\n",
    "# and fill empty space with 'padword' and label 'O' in sentences, sen_tag, respectively\n",
    "max_len = int(np.percentile(sen_lengths, 95))\n",
    "def fill_padword(sequences, max_length, pad):\n",
    "    for i, seq in enumerate(sequences):\n",
    "        if len(seq) < max_length:\n",
    "            add_pad = [pad] * (max_length - len(seq))\n",
    "            sequences[i] = seq + add_pad\n",
    "        else:\n",
    "            sequences[i] = seq[:max_length]\n",
    "    return sequences\n",
    "\n",
    "sentences = fill_padword(sentences, max_len, \"padword\")\n",
    "sen_tag = fill_padword(sen_tag, max_len, tag2num[\"O\"])            \n",
    "sentences[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 7, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen_tag[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding for labels (1001, 30) -> (1001, 30, 9)\n",
    "y = np.array(sen_tag)\n",
    "y = [to_categorical(i, num_classes= len(n_tags)) for i in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 777,  739,  698,  830,  532,  389,  684,  929, 1208, 1208, 1208,\n",
       "       1208, 1208, 1208, 1208, 1208, 1208, 1208, 1208])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert sequences of words into sequences of numbers, each number corresponds to a word in corpus\n",
    "corpus2num = dict([word, i] for i, word in enumerate(corpus))\n",
    "def seqword2seqnum(sequence, word2num):\n",
    "    for i, word in enumerate(sequence):\n",
    "        sequence[i] = word2num[word]\n",
    "    return sequence\n",
    "sentences = [seqword2seqnum(sequence, corpus2num) for sequence in sentences]\n",
    "sentences = np.array(sentences)\n",
    "#sen_tag = np.array(sen_tag)\n",
    "sentences[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use word2vec file for embedding for words in corpus\n",
    "def get_coefs(word, *arr): \n",
    "    return word, np.asarray(arr, dtype = 'float32')\n",
    "embedding = dict(get_coefs(*o.strip().split(\"\\t\")) for o in \n",
    "                 open(word2vec_dir, encoding = 'utf-8', errors ='ignore').read().split('\\n'))\n",
    "\n",
    "dimension = 300\n",
    "embedding.pop('') # drop key ''\n",
    "mean = np.stack(embedding.values(), axis = 1).mean()\n",
    "std = np.stack(embedding.values(), axis =1).std()\n",
    "embed_weights = np.random.normal(mean, std, size = (len(corpus), dimension))\n",
    "\n",
    "for word, i in corpus2num.items():\n",
    "    if embedding.get(word) is not None:\n",
    "        embed_weights[i] = embedding.get(word)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training 90% and test 10%.\n",
    "train_X, test_X, train_y, test_y = train_test_split(sentences, np.array(y), test_size = 0.1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build training and validation generator \n",
    " \n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, sentences, y, batch_size = 16, shuffle = True, num_class = 10):\n",
    "        self.sentences = sentences\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        #self.dim = dim\n",
    "        self.num_class = num_class\n",
    "        self.shuffle = shuffle\n",
    "        self.shuffle_indexes()\n",
    "    def shuffle_indexes(self):\n",
    "        self.indexes = np.arange(len(self.y))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.y) / self.batch_size))\n",
    "    def __getitem__(self, index):\n",
    "        ind = self.indexes[(index*self.batch_size) : np.min([(index+1)*self.batch_size, len(self.y)])] \n",
    "        data = [self.sentences[i] for i in ind]\n",
    "        label = [self.y[i] for i in ind]\n",
    "        #y = to_categorical(y, num_classes = self.num_class)\n",
    "        return np.array(data), np.array(label).astype('int32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57 batchs of data in training generator\n"
     ]
    }
   ],
   "source": [
    "parameters = {'batch_size': 16,\n",
    "              'shuffle': True,\n",
    "              'num_class': len(n_tags)}\n",
    "train_generator = DataGenerator(train_X, train_y,**parameters)\n",
    "validation_generator = DataGenerator(test_X, test_y,**parameters)\n",
    "#generator.__getitem__(62)\n",
    "print(\"{:} batchs of data in training generator\".format(train_generator.__len__()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now onto neural network architecture\n",
    "def build_model(unit1 = 128, unit2 = 128, recurrent_dropout = 0.1, dropout = 0.1, epochs = 5, filepath = \"best_model.h5\"):\n",
    "    \n",
    "    callback_list = [keras.callbacks.EarlyStopping(monitor = 'val_acc', patience = 2), \n",
    "                     keras.callbacks.ModelCheckpoint(filepath = filepath, monitor = 'val_acc',\n",
    "                                                     save_best_only = True)]\n",
    "    input_ = Input(shape=(max_len,))\n",
    "    embedding = Embedding(len(corpus), dimension, weights = [embed_weights], trainable = False)(input_)\n",
    "    x = Bidirectional(LSTM(units = unit1, return_sequences=True,\n",
    "                           recurrent_dropout= recurrent_dropout, dropout = dropout))(embedding)\n",
    "    x_rnn = Bidirectional(LSTM(units = unit2, return_sequences=True,\n",
    "                               recurrent_dropout = recurrent_dropout, dropout = dropout))(x)\n",
    "    x = concatenate([x, x_rnn])  # residual connection to the first biLSTM\n",
    "    output_ = TimeDistributed(Dense(len(n_tags), activation=\"softmax\"))(x)\n",
    "    model = Model(input_, output_)\n",
    "    \n",
    "    model.summary()\n",
    "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    \n",
    "    model.fit_generator(train_generator, \n",
    "                        epochs = epochs, \n",
    "                        steps_per_epoch = len(train_generator),\n",
    "                        validation_data = validation_generator,\n",
    "                        validation_steps = len(validation_generator),\n",
    "                        callbacks = callback_list\n",
    "                        )\n",
    "    model.load_weights(filepath)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 19)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 19, 300)      431100      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 19, 1024)     3330048     embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 19, 1024)     6295552     bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 19, 2048)     0           bidirectional_1[0][0]            \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 19, 9)        18441       concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 10,075,141\n",
      "Trainable params: 9,644,041\n",
      "Non-trainable params: 431,100\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/5\n",
      "57/57 [==============================] - 68s 1s/step - loss: 0.8446 - acc: 0.7332 - val_loss: 0.4300 - val_acc: 0.8692\n",
      "Epoch 2/5\n",
      "57/57 [==============================] - 66s 1s/step - loss: 0.2716 - acc: 0.9107 - val_loss: 0.1237 - val_acc: 0.9640\n",
      "Epoch 3/5\n",
      "57/57 [==============================] - 69s 1s/step - loss: 0.1157 - acc: 0.9630 - val_loss: 0.0737 - val_acc: 0.9781\n",
      "Epoch 4/5\n",
      "57/57 [==============================] - 67s 1s/step - loss: 0.0579 - acc: 0.9810 - val_loss: 0.0375 - val_acc: 0.9870\n",
      "Epoch 5/5\n",
      "57/57 [==============================] - 67s 1s/step - loss: 0.0385 - acc: 0.9877 - val_loss: 0.0267 - val_acc: 0.9911\n"
     ]
    }
   ],
   "source": [
    "model = build_model(unit1 = 512, unit2 = 512, recurrent_dropout = 0.2, dropout = 0.2, epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.99\n",
      "precision: 0.9\n",
      "recall: 0.94\n",
      "f1: 0.92\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   NEWSTYPE       1.00      1.00      1.00        74\n",
      "   PROVIDER       0.88      0.95      0.91        56\n",
      "   KEYWORDS       0.84      0.90      0.87        77\n",
      "    SECTION       0.87      0.81      0.84        16\n",
      "\n",
      "avg / total       0.91      0.94      0.92       223\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model by inspecting following measures: precision, recall, f1\n",
    "val_pred = model.predict(test_X, batch_size = 16)\n",
    "def pred2label(pred):\n",
    "    labels = []\n",
    "    for sentence in pred:\n",
    "        label = []\n",
    "        for word in sentence:\n",
    "            index = np.argmax(word, axis = -1)\n",
    "            label.append(num2tag[index])\n",
    "        labels.append(label)\n",
    "    return labels\n",
    "pred_labels = pred2label(val_pred)\n",
    "true_labels = pred2label(test_y)\n",
    "\n",
    "def evaluate(y_true, y_pred):\n",
    "    print(\"accuracy: {:.2}\".format(accuracy_score(y_true, y_pred)))\n",
    "    print(\"precision: {:.2}\".format(precision_score(y_true, y_pred)))\n",
    "    print(\"recall: {:.2}\".format(recall_score(y_true, y_pred)))\n",
    "    print(\"f1: {:.2}\".format(f1_score(y_true, y_pred)))\n",
    "    print(classification_report(y_true, y_pred))\n",
    "\n",
    "evaluate(true_labels, pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Predict IOB tags for new queries'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(query):\n",
    "    return re.findall(\"[\\w']+|[!#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]\", query.lower())\n",
    "\n",
    "def out_domain_tokens(tokens):\n",
    "    out_domain = []\n",
    "    for word in tokens:\n",
    "        if word.lower() not in corpus:\n",
    "            out_domain.append(word)\n",
    "    return list(set(out_domain))\n",
    "\n",
    "#out_domain_tokens(text)\n",
    "\n",
    "# break query at characters of ',' and '.'\n",
    "def form_subquery(tokens):\n",
    "\n",
    "    period_ind = [i for i, token in enumerate(tokens) if token=='.']\n",
    "    comma_ind = [i for i, token in enumerate(tokens) if token==',']\n",
    "    period_ind.extend(comma_ind)\n",
    "    breaks = sorted(period_ind)\n",
    "    sub_queries = [\" \".join(tokens[i : j]).replace('.','').replace(',','').strip() \n",
    "                   for i, j in zip([0] + breaks, breaks + [None])]\n",
    "    if '' in sub_queries:\n",
    "        sub_queries.remove(\"\")\n",
    "    return sub_queries\n",
    "\n",
    "# remove words out of new domain from these sub-queries\n",
    "def remove_out_domain(subs, out_domain):\n",
    "    for i, sub in enumerate(subs):\n",
    "        sub_seq = sub.split()\n",
    "        for sub_word in sub.split():\n",
    "           if sub_word in out_domain:\n",
    "               sub_seq.remove(sub_word)\n",
    "        subs[i] = \" \".join(sub_seq)\n",
    "    return subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets deal with long subquery: those whose number of words excced max_len\n",
    "def long_query(queries, max_length, back = 5, sentence = True):\n",
    "    copy = []\n",
    "    cut = []\n",
    "    for query in queries:\n",
    "        \n",
    "        if len(query.split()) <= max_length:\n",
    "            if sentence:\n",
    "                copy.append(query)\n",
    "            else:\n",
    "                copy.append(query.split())\n",
    "            cut.append((0, len(query.split())))\n",
    "     \n",
    "        else:\n",
    "            querylen = len(query.split())\n",
    "            end = max_length\n",
    "            cut.append((0,end)) \n",
    "            while end <= querylen:\n",
    "                if sentence:\n",
    "                    sub = \" \".join(query.split()[end - max_length : end])\n",
    "                else:\n",
    "                    sub = query.split()[end - max_length : end]\n",
    "                copy.append(sub)\n",
    "                end = end + max_length - back\n",
    "                if end <= querylen:\n",
    "                    cut.append((back, None))\n",
    "                \n",
    "            if sentence: \n",
    "                sub = \" \".join(query.split()[end - max_length : ])\n",
    "            else:\n",
    "                sub = query.split()[end - max_length : ]\n",
    "            \n",
    "            copy.append(sub)\n",
    "            cut.append((back, querylen + max_length - end))\n",
    "\n",
    "    return copy, cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an example of dealing with long queries\n",
    "sample = 'crimean bissau @kingsport %annarborcom  dr sicilia, mbabane smith huron kingston < paul boni island ! agritech whale eruption# windsor with 360 kate táº¥n turkey. beckinsale oskaloosa poliomyelitis burns international queen chile kokhir junta next reporter verde breeze huston comics freescale vienna cultural hemsworth hamilton park microsoft smoky mediterranean tobago el flood canadian. pettyfer tire blizzard brownwood community aquion eccentric gaceta nursultan californian poughkeepsie fever daviÃ° state brazil lapaglia noumea brightsource appeal wave savarin far film haider redford crier guatemala rockslide higgins holly tunisia serbia rica electronics missoulian fairfield sopoaga. brand samoa lance rooney livonia wyle austin typhus, technologies liechtenstein glendale israel browne.'\n",
    "tokens = get_tokens(sample)        \n",
    "out_domain = out_domain_tokens(tokens)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['crimean bissau @ kingsport % annarborcom dr sicilia',\n",
       " 'mbabane smith huron kingston < paul boni island ! agritech whale eruption # windsor with 360 kate táº n turkey',\n",
       " 'beckinsale oskaloosa poliomyelitis burns international queen chile kokhir junta next reporter verde breeze huston comics freescale vienna cultural hemsworth hamilton park microsoft smoky mediterranean tobago el flood canadian',\n",
       " 'pettyfer tire blizzard brownwood community aquion eccentric gaceta nursultan californian poughkeepsie fever daviã state brazil lapaglia noumea brightsource appeal wave savarin far film haider redford crier guatemala rockslide higgins holly tunisia serbia rica electronics missoulian fairfield sopoaga',\n",
       " 'brand samoa lance rooney livonia wyle austin typhus',\n",
       " 'technologies liechtenstein glendale israel browne']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cut query into sentences based on the appearance of ',' and '.'\n",
    "subs = form_subquery(tokens)\n",
    "subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['crimean bissau kingsport annarborcom dr sicilia',\n",
       " 'mbabane smith huron kingston paul boni island agritech whale eruption windsor with 360 kate n turkey',\n",
       " 'beckinsale oskaloosa poliomyelitis burns international queen chile kokhir junta next reporter verde breeze huston comics freescale vienna cultural hemsworth',\n",
       " 'comics freescale vienna cultural hemsworth hamilton park microsoft smoky mediterranean tobago el flood canadian',\n",
       " 'pettyfer tire blizzard brownwood community aquion eccentric gaceta nursultan californian poughkeepsie fever state brazil lapaglia noumea brightsource appeal wave',\n",
       " 'lapaglia noumea brightsource appeal wave savarin far film haider redford crier guatemala rockslide higgins holly tunisia serbia rica electronics',\n",
       " 'holly tunisia serbia rica electronics missoulian fairfield sopoaga',\n",
       " 'brand samoa lance rooney livonia wyle austin typhus',\n",
       " 'technologies liechtenstein glendale israel browne']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove tokens from sentences, and break sentences longer than maxlen into sub-sentences\n",
    "maxlen = 19\n",
    "clean_subs = remove_out_domain(subs, out_domain)\n",
    "break_subs, cut = long_query(clean_subs, max_length = maxlen, sentence = True) \n",
    "break_subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(prediction):\n",
    "\n",
    "        print(\"\\n\")\n",
    "        print(29 * '-')\n",
    "        print(\"{:15} | {:}\".format(\"Word\", \"Pred_label\"))\n",
    "        print(29 * '-')\n",
    "        for i, word, pre in prediction:\n",
    "            print(\"{:15} | {:}\".format(word, pre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(query, maxlen):\n",
    "    \n",
    "    tokens = get_tokens(query)        \n",
    "    out_domain = out_domain_tokens(tokens) \n",
    "    subs = form_subquery(tokens)\n",
    "    \n",
    "    clean_subs = remove_out_domain(subs, out_domain)\n",
    "    break_subs, cut = long_query(clean_subs, max_length = maxlen, sentence = False)    \n",
    "    \n",
    "    sent = fill_padword(break_subs, maxlen, \"padword\")\n",
    "    sent = [seqword2seqnum(sequence, corpus2num) for sequence in sent]\n",
    "\n",
    "    A_pred = model.predict(np.array(sent), batch_size = 1)\n",
    "    A_tag = [[num2tag[i] for i in index] for index in np.argmax(A_pred, axis = -1)]\n",
    "    \n",
    "    align = []\n",
    "    for sub, t in zip(A_tag, cut):\n",
    "        align += sub[t[0]:t[1]]\n",
    "    \n",
    "    token2tag = [(token, 'O') if token in out_domain else (token, '') for token in tokens]\n",
    "    \n",
    "    in_do = []\n",
    "    out_do = []\n",
    "    j = 0\n",
    "    for i, t in enumerate(token2tag):\n",
    "    \n",
    "        if t[0] in out_domain:\n",
    "            out_do.append((i, t[0], t[1]))\n",
    "        else:\n",
    "            in_do.append((i, t[0], align[j]))\n",
    "            j += 1    \n",
    "    \n",
    "    final = in_do + out_do\n",
    "    final.sort()\n",
    "    \n",
    "    #sentence = \"windsor central bank would want sports a commercial outlook\"\n",
    "    \n",
    "    display(final) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-----------------------------\n",
      "Word            | Pred_label\n",
      "-----------------------------\n",
      "i               | O\n",
      "want            | O\n",
      "to              | O\n",
      "travel          | O\n",
      "$               | O\n",
      "%               | O\n",
      "around          | O\n",
      "the             | O\n",
      "#               | O\n",
      "world           | B-KEYWORDS\n",
      "from            | O\n",
      "a               | O\n",
      "canadian        | B-PROVIDER\n",
      "city            | I-PROVIDER\n",
      "to              | O\n",
      "new             | O\n",
      "york            | I-PROVIDER\n",
      "view            | O\n",
      "financial       | I-KEYWORDS\n",
      "company         | I-KEYWORDS\n",
      ".               | O\n",
      "from            | O\n",
      "new             | B-PROVIDER\n",
      "york            | I-PROVIDER\n",
      ",               | O\n",
      "i               | O\n",
      "travel          | I-KEYWORDS\n",
      "west            | I-KEYWORDS\n",
      "to              | O\n",
      "visit           | O\n",
      "my              | O\n",
      "sports          | B-PROVIDER\n",
      "star            | I-PROVIDER\n",
      "in              | O\n",
      "philadelphia    | B-PROVIDER\n",
      "and             | I-PROVIDER\n",
      "watch           | O\n",
      "a               | O\n",
      "rock            | B-PROVIDER\n",
      "show            | I-PROVIDER\n",
      "then            | O\n",
      "take            | O\n",
      "a               | O\n",
      "train           | B-KEYWORDS\n",
      "to              | I-KEYWORDS\n",
      "west            | I-KEYWORDS\n",
      "coast           | I-KEYWORDS\n",
      "to              | O\n",
      "enjoy           | O\n",
      "the             | O\n",
      "pacific         | B-PROVIDER\n",
      "weather         | I-PROVIDER\n"
     ]
    }
   ],
   "source": [
    "main(\"I want to travel $ %around the# world from a Canadian city to New York view financial company. From New York, I travel west to visit my sports star in Philadelphia and watch a Rock show then take a train to west coast to enjoy the pacific weather\", max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
